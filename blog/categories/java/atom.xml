<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Java | Chris's Blog]]></title>
  <link href="http://askcuix.github.io/blog/categories/java/atom.xml" rel="self"/>
  <link href="http://askcuix.github.io/"/>
  <updated>2013-06-19T23:46:15+08:00</updated>
  <id>http://askcuix.github.io/</id>
  <author>
    <name><![CDATA[Chris]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[XML文件比较工具]]></title>
    <link href="http://askcuix.github.io/blog/2013/06/01/the-tool-for-xml-compare/"/>
    <updated>2013-06-01T17:27:00+08:00</updated>
    <id>http://askcuix.github.io/blog/2013/06/01/the-tool-for-xml-compare</id>
    <content type="html"><![CDATA[<p>最近公司的新产品准备上线，因为涉及到多个系统之间的集成，所以要做几轮的模拟测试，以保证系统间的稳定运行以及数据的准确性。对于其中一个Batch Job，我们需要生成多种XML给其它系统使用，这个Batch并不是全新的，在之前的版本就有，但是这次数据模型有变动，一些业务逻辑也有少许变动，但期望生成的数据是一样的，所以就需要比较两个版本的系统所生成的XML。</p>

<p>现在真是越来越不喜欢XML，超过30M的文件，基本就不可能用NotePad++打开了（与我的机器配置太差也有关系）。通常对于一些小的文件，还可以用Beyond Compare之类的比较工具，而我们Batch生成的XML文件可能会超过100M，那就不可能通过一些桌面软件去做比较了，而且数量太大，肉眼比较的准确性也不能保证。<!-- more -->这些XML文件中主要是金融产品信息，由于在生成文件的时候并没有做排序，因此每次生成的文件中，产品的顺序可能也是不同的，这样就更难用比较工具了。</p>

<p>对于这种情况，只能自己去写个比较工具了，在网上找了一下，也没找到满足要求的，现在代码已经完成，放在<a href="https://github.com/askcuix/xml-compare-tool">GitHub</a>,可以生成比较详细的统计信息，如XML中的record数量，不同record的数量，不相同的field，处理时间等。在我那台破机器上，比较两个150M的文件，可以在两分钟内完成。</p>

<p>具体的用法可项目的主页上已经有描述了，这里简单说下实现的思路，因为实现的比较匆忙，有很多考虑不周的地方。实现的步骤如下：</p>

<ol>
<li>通过SAX解析将源文件解析成一个Map，key为产品ID，可以为一个或多个element组合而成，value为该产品所对应的XML内容。</li>
<li>仍然通过SAX解析待比较的文件，每解析出一个产品信息，就从之前的Map中取出相同的产品信息，这时就得到了相同产品的两个版本的内容。如果产品只存在于待比较的文件中，则计入不同record的数量。</li>
<li>使用DOM解析两个XML片段，结果放在两个Map中，key为element的名字，value的element的值。这里需要考虑几种情况：如果element是可选的，则可能只出现在其中一个XML片段中；如果element是重复性的组合元素，则当作一个field。</li>
<li>在比较的过程中，每比较完一个XML片段就删除引用，以达到在内存不够时，可以通过GC释放内存。</li>
<li>当比较结束，第一步产生的Map中若还有产品存在，则为待比较的文件中没有的产品，计入不同record的数量。</li>
<li>统计比较结果，一并写入文件中。</li>
</ol>


<p>暂时未考虑到更好的有效利用内存的方法，对于比较重复性的element，现在是通过配置来识别，一直在纠结要不要通过解析XSD来实现，但这种方式既增加了代码的复杂性，也很难去处理不同级别的重复性元素，比如root element是productList，下一级就是重复性的元素product,对于这种情况，是不应当视为重复性元素来处理的。</p>

<p>改进还要继续，提供给更多有相似需求的人使用。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Spring NamedParameterJdbcTemplate的内存泄漏问题]]></title>
    <link href="http://askcuix.github.io/blog/2013/05/06/the-memory-leak-problem-of-spring-namedparameterjdbctemplate/"/>
    <updated>2013-05-06T22:36:00+08:00</updated>
    <id>http://askcuix.github.io/blog/2013/05/06/the-memory-leak-problem-of-spring-namedparameterjdbctemplate</id>
    <content type="html"><![CDATA[<p>我们的项目是使用Spring JDBC来操作DB的，通常是直接使用SimpleJdbcTemplate，一直以来也没发现什么问题。今天在做performance test的时候，发现内存增长很快，甚至出现了out of memory。把heap dump拉下来查看后，发现是这次新加的一个service出现了问题，但根源是因为NamedParameterJdbcTemplate中的HashMap导致的。</p>

<p>这里要先说明使用的Spring的版本是2.5.6，在3.0.3中已修复该问题。</p>

<!-- more -->


<p>查看调用的trace是service->SimpleJdbcTemplate->NamedParameterJdbcTemplate。一直没有仔细看过SimpleJdbcTemplate的源码，其实它基本都是通过调用NamedParameterJdbcTemplate来完成操作的。SimpleJdbcTemplate的初始化，其实就是初始化NamedParameterJdbcTemplate。</p>

<p>``` java SimpleJdbcTemplate.java
   public SimpleJdbcTemplate(DataSource dataSource) {</p>

<pre><code>    this.namedParameterJdbcOperations = new NamedParameterJdbcTemplate(dataSource);
</code></pre>

<p>   }</p>

<p>   public SimpleJdbcTemplate(JdbcOperations classicJdbcTemplate) {</p>

<pre><code>    this.namedParameterJdbcOperations = new NamedParameterJdbcTemplate(classicJdbcTemplate);
</code></pre>

<p>   }</p>

<p>   public SimpleJdbcTemplate(NamedParameterJdbcOperations namedParameterJdbcTemplate) {</p>

<pre><code>    this.namedParameterJdbcOperations = namedParameterJdbcTemplate;
</code></pre>

<p>   }
```</p>

<p>NamedParameterJdbcTemplate的作用是方便使用命名式的参数，以代替使用SQL中的‘?’。NamedParameterJdbcTemplate会将每次解析过后的SQL放在一个HashMap中，以起到cache的作用，而问题就出现这个Map上。</p>

<p>``` java NamedParameterJdbcTemplate.java
   private final Map parsedSqlCache = new HashMap();</p>

<p>   protected ParsedSql getParsedSql(String sql) {</p>

<pre><code>    synchronized (this.parsedSqlCache) {
        ParsedSql parsedSql = (ParsedSql) this.parsedSqlCache.get(sql);
        if (parsedSql == null) {
            parsedSql = NamedParameterUtils.parseSqlStatement(sql);
            this.parsedSqlCache.put(sql, parsedSql);
        }
        return parsedSql;
    }
}
</code></pre>

<p>```</p>

<p>从上面的代码可以看到，NamedParameterJdbcTemplate不断的将解析过的SQL放入parsedSqlCache中，但并没有任何限制，而我们的service会根据不同的条件产生不同的SQL（条件参数比较多），同时这个SimpleJdbcTemplate也是share的，因此出现了out of memory的问题，当然，测试server的配置比较差也是其中一个原因。</p>

<p>这个问题可以参见<a href="https://jira.springsource.org/browse/SPR-7237">Spring Bug 7237</a>，该问题已在3.0.3中修复。</p>

<p>由于该阶段已不可能升级Spring的版本，潜在的风险太大，因此可自己实现一个NamedParameterJdbcTemplate，去override掉getParsedSql方法，可取消cache的作用，或参考Spring 3中的实现。</p>

<p>``` java NamedParameterJdbcTemplate.java
   public static final int DEFAULT_CACHE_LIMIT = 256;</p>

<p>   private volatile int cacheLimit = DEFAULT_CACHE_LIMIT;</p>

<p>   private final Map&lt;String, ParsedSql> parsedSqlCache =</p>

<pre><code>        new LinkedHashMap&lt;String, ParsedSql&gt;(DEFAULT_CACHE_LIMIT, 0.75f, true) {
  @Override
  protected boolean removeEldestEntry(Map.Entry&lt;String, ParsedSql&gt; eldest) {
        return size() &gt; getCacheLimit();
  }
};
</code></pre>

<p>   public void setCacheLimit(int cacheLimit) {</p>

<pre><code>    this.cacheLimit = cacheLimit;
</code></pre>

<p>   }</p>

<p>   public int getCacheLimit() {</p>

<pre><code>    return this.cacheLimit;
</code></pre>

<p>   }</p>

<p>   protected ParsedSql getParsedSql(String sql) {</p>

<pre><code>    if (getCacheLimit() &lt;= 0) {
        return NamedParameterUtils.parseSqlStatement(sql);
    }
    synchronized (this.parsedSqlCache) {
        ParsedSql parsedSql = this.parsedSqlCache.get(sql);
        if (parsedSql == null) {
            parsedSql = NamedParameterUtils.parseSqlStatement(sql);
            this.parsedSqlCache.put(sql, parsedSql);
        }
        return parsedSql;
    }
</code></pre>

<p>   }
```</p>

<p>然后更改一下SimpleJdbcTemplate的实例化方式，先实例化一个改写过的NamedParameterJdbcTemplate，然后作为SimpleJdbcTemplate构造函数的参数来进行实例化。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[垃圾收集器及内存分配]]></title>
    <link href="http://askcuix.github.io/blog/2013/04/16/garbage-collection-and-memory-allocation/"/>
    <updated>2013-04-16T23:54:00+08:00</updated>
    <id>http://askcuix.github.io/blog/2013/04/16/garbage-collection-and-memory-allocation</id>
    <content type="html"><![CDATA[<p>程序计数器、虚拟机栈和本地方法栈的内存分配大体在编译器就可以确定，并且这些区域和线程有着相同的生命周期，在方法或线程结束时，内存也就回收了。而Java堆和方法区的内存分配和回收都是动态的，只有在程序运行期间才知道会创建哪些对象，因此是垃圾收集器所关注的内存区域。</p>

<h2>对象标记算法</h2>

<p>Java堆中存放着所有对象实例，垃圾收集器在进行回收前，需要根据算法来确定哪些对象已不再被使用，可进行回收。</p>

<h3>引用计数算法</h3>

<p>该算法是给对象添加一个引用计数器，每当有引用它时，计数器就加1；当引用失效时，计数器就减1；当计数器为0时就不能被使用了。这种算法的判定效率很高，但是很难解决Java对象之间的循环引用问题，因此未被Java语言采用。</p>

<h3>根搜索算法</h3>

<p>通过一系列的名为“GC Roots”的对象作为起始点，从这些节点开始向下搜索，搜索所走过的路径称为引用链，当一个对象当GC Roots没有任何引用链相连时，则此对象就不能被使用了。</p>

<p>可作为GC Roots的对象包括：
- 虚拟机栈中的局部变量表所引用的对象。 <br/>
- 方法区中的类静态属性引用的对象。  <br/>
- 方法区中的常量引用的对象。 <br/>
- 本地方法栈中JNI所引用的对象。</p>

<!-- more -->


<h2>Java引用</h2>

<p><strong>强引用</strong>：在代码中普遍存在的，比如“Object obj = new Object()”，只要强引用还存在，垃圾收集器则永远都不会回收引用的对象。</p>

<p><strong>软引用</strong>：一些还有用，但并非必须的对象。对于软引用关联的对象，在发生内存溢出之前，会将这些对象纳入回收范围并进行回收。</p>

<p><strong>弱引用</strong>：非必须对象，比软引用更弱一些，弱引用所关联的对象只能生存到下一次垃圾回收之前。当垃圾收集器工作时，无论当前内存是否够用，都会回收弱引用所关联的对象。</p>

<p><strong>虚引用</strong>：最弱的一种引用关系，一个对象是否被虚引用所关联，完全不会对其生存时间产生影响，也不能通过虚引用来取得对象实例，关联的唯一目的是在这个对象被垃圾回收器回收时收到一个系统通知。</p>

<h2>垃圾收集算法</h2>

<h3>标记-清除算法</h3>

<p>首先标记出所有需要回收的对象，之后统一回收所有被标记的对象。该算法效率较低，并会产生大量不连续的内存碎片。</p>

<h3>复制算法</h3>

<p>将新生代内存分为一块较大的Eden空间和两块较小的Survivor空间，每次使用Eden和其中一块Survivor，回收时将Eden和使用的Survivor中存活的对象一次性拷贝到另一块Survivor空间，然后清理Eden和之前使用的Survivor空间。</p>

<p>HotSpot虚拟机默认Eden和Survivor的大小比例是8:1，因此新生代中可用内存为整个新生代容量的90%。当Survivor空间不够用时，将依赖老年代内存进行分配担保。</p>

<h3>标记-整理算法</h3>

<p>与标记-清除算法类似，但最后步骤不是直接对可回收对象进行清除，而是将所有存活的对象都向一端移动，然后直接清理掉端边界以外的内存。</p>

<h3>分代收集算法</h3>

<p>将Java堆分为新生代和老年代，对每个年代采用合适的收集算法。</p>

<h2>垃圾收集器</h2>

<h3>Serial收集器</h3>

<p>单线程新生代收集器，在进行垃圾收集时，必须暂停所有的工作线程，导致停顿。与其它收集器的单线程相比简单高效，对于单CPU的环境没有线程交互的开销，收集效率较高。</p>

<h3>ParNew收集器</h3>

<p>新生代收集器，Serial收集器的多线程版本，是目前除Serial收集器外，唯一能够和CMS收集器配合工作的。默认开启的收集线程数与CPU的数量相同，可通过-XX:ParallelGCThreads来限制线程数。</p>

<p>ParNew收集器是使用-XX:+UseConcMarkSweepGC后的默认新生代收集器，也可以使用-XX:+UseParNewGC来强制指定。</p>

<h3>Parallel Scavenge收集器</h3>

<p>新生代收集器，以达到可控制的吞吐量为目标。可通过-XX:MaxGCPauseMillis来控制最大垃圾收集停顿时间，-XX:GCTimeRatio来控制吞吐量大小，-XX:+UseAdaptiveSizePolicy来实现自适应调节策略。</p>

<h3>Serial Old收集器</h3>

<p>Serial收集器的老年代版本。可在JDK1.5及之前版本中配合Parallel Scavenge收集器使用，也可在CMS收集器发生Concurrent Mode Failure时使用。</p>

<h3>Parallel Old收集器</h3>

<p>Parallel Scavenge收集器的老年代版本。使用多线程和“标记-整理”算法，在JDK1.6中开始提供。和Parallel Scavenge收集器配合使用非常适合注重吞吐量和CPU资源有限的场景。</p>

<h3>CMS（Concurrent Mark Sweep）收集器</h3>

<p>老年代收集器，以获取最短回收停顿时间为目标。默认启动的收集线程数是 (CPU数量+3) / 4。在并发阶段因占用一部分线程而导致应用程序变慢，总吞吐量下降。CMS收集器默认在老年代使用68%的空间后被激活，可通过调高-XX:CMSInitiatingOccupancyFraction来提高触发百分比，以降低内存回收次数。</p>

<p>若CMS运行期间预留的内存无法满足程序需要，就会出现“Concurrent Mode Failure”失败，虚拟机会临时启用Serial Old收集器来重新进行老年代的垃圾收集。</p>

<p>由于使用“标记-清除”算法会产生大量空间碎片，从而导致Full GC，可通过-XX:+UseCMSCompactAtFullCollection在Full GC后进行碎片整理。</p>

<h3>G1收集器</h3>

<p>使用“标记-整理”算法避免产生空间碎片，可以精确控制垃圾收集的时间。通过将整个Java堆划分为多个大小固定的独立区域，每次根据允许的收集时间，优先回收垃圾最多的区域，以此来保证在有限的时间内获取最高的收集效率。</p>

<h2>内存分配</h2>

<p>对象通常在新生代Eden区中分配，当Eden区没有足够空间时，将触发一次Minor GC。</p>

<p>需要大量连续内存的大对象将直接在老年代中分配，以避免新生代中发生大量的内存拷贝，可通过-XX:PretenureSizeThreshold来设置。</p>

<p>虚拟机为每个对象定义了年龄计数器，并通过Minor GC的次数来增加年龄，当达到-XX:MaxTenuringThreshold设置的阀值时，会将对象移入老年代。</p>

<p>若新生代中Survivor空间中相同年龄的对象大小总和大于Survivor空间的一半，则年龄大于或等于该年龄的对象可以直接进入老年代。</p>

<p><strong>新生代GC（Minor GC）</strong>：新生代的垃圾回收操作，Java对象的生命周期通常较短，因此Minor GC非常频繁。 <br/>
<strong>老年代GC（Major GC / Full GC）</strong>：老年代的垃圾回收操作，发生Full GC通常也会发生至少一次的Minor GC，GC的速度很慢。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[The foundation of spring batch]]></title>
    <link href="http://askcuix.github.io/blog/2013/04/01/the-foundation-of-spring-batch/"/>
    <updated>2013-04-01T22:46:00+08:00</updated>
    <id>http://askcuix.github.io/blog/2013/04/01/the-foundation-of-spring-batch</id>
    <content type="html"><![CDATA[<p>Spring Batch是埃森哲贡献给Spring的一个开源项目，现在由双方共同维护。通过Spring Batch可以构建出轻量级的大数据量并⾏处理应⽤,⽀持事务、并发、流程、监控、纵向和横向扩展,提供统一的接口管理。Spring Batch并不包含scheduler，它只是一个通用的batch处理框架，你可以通过QuartZ，Control-M等scheduler去调用。</p>

<p>Spring Batch能够处理大批量数据的导入、导出和业务逻辑计算，执行过程无需人工干预。我们的系统是一个金融产品信息的数据中心，需要从不同的系统读取产品信息，也需要将这些信息提供给其它的系统，这种data loading就是Spring Batch的一种应用场景。</p>

<!-- more -->


<p><img src="/images/blog/spring_batch_common_process.png" alt="Spring Batch Common Process" /></p>

<p>上图就是Spring Batch的常用方式，从DB或是不同类型的文件中读取数据，经过转换后再写入DB或者文件。</p>

<p><img src="/images/blog/spring_batch_domain.png" alt="Spring Batch Domain" /></p>

<p>上图是一个Job执行的结构图，下面是对这些domain对象的简单描述，只是学习时随笔记下来的，不够详细，可以查看Spring Batch文档中的描述。</p>

<h2>Job</h2>

<p>Job是对一个batch处理流程的定义，可以简单理解为Spring中的一个Job配置，一般都包括reader和writer。</p>

<h2>JobInstance</h2>

<p>JobInstance是Job运行时产生的实例，如果这个Job是按天执行的，则每天都会创建一个JobInstance；当Job执行失败时，restart这个Job将会重用JobInstance，以便于从执行失败的地方开始重新执行。</p>

<h2>JobParameter</h2>

<p>JobParameter用于标识一个JobInstance，可以将它理解为JobInstance的ID。</p>

<h2>JobListener</h2>

<p>JobListener可以监听JobInstance生命周期中的两个事件：</p>

<p><strong>beforeJob</strong> - 在Job执行前被调用。</p>

<p><strong>afterJob</strong> - 不论Job执行成功或失败都会被调用，Job的status可以从JobExecution中获取。</p>

<h2>Parent Job</h2>

<p>对于相似的Job，可以取出相同的部分定义成一个parent Job，只需要在Job定义时添加<em>abstract="true"</em>，然后在sub Job中通过<em>parent</em>属性来指定parent Job。</p>

<h2>JobRepository</h2>

<p>MapJobRepository通常用于测试环境或是standalone的batch。它不够稳定；不允许在不同JVM实例之间执行restart；也不能保证有相同JobParameter的两个JobInstance同时运行；它不适合在多线程Job或本地的partition step中使用。但是它仍然需要配置transaction manager，因为在Job的实现中会涉及到rollback的问题，所以可以用ResourcelessTransactionManager来代替。</p>

<h2>JobLauncher</h2>

<p>JobLauncher使用spring的taskExecutor来实现异步处理，只需要在配置中指定taskExecutor属性。如果batch是通过http request的方式触发的，那应当使用异步的方式来处理Job以避免长时间的占用链接。</p>

<h2>JobOperator</h2>

<p>JobOperator的stop()不会立刻停止一个Job，如果当前流程处理的控制权在framework，则会将StepExecution的status设置为STOPPED并保存，然后按正常的处理流程结束Job。</p>

<p>如果不希望一个restartable的Job在执行失败后restart，可以将status设置为ABORTED。</p>

<h2>JobParametersIncrementer</h2>

<p>JobParametersIncrementer可强制创建一个新的jobInstance，以避免在使用相同JobParameter时不可以再次执行Job。这只是个接口，需要自己去实现getNext()，然后在Job中定义incremented属性来引用它。</p>

<h2>Step</h2>

<p><strong>start-limit</strong>： 用于step的restart，用来控制一个start的次数。默认是Integer.MAX_VALUE。<br/>
<strong>allow-start-if-complete</strong>： 强制执行step，不管之前执行成功或失败。</p>

<h3>Skip</h3>

<p>通过指定exception来决定是否skip有问题的数据，可以结合include和exclude来定义exception列表。</p>

<p><strong>skip-limit</strong>：用来控制允许skip的数据的最大数量，在step execution中分别保存有针对read，process和write中skip的数量。</p>

<h3>Retry</h3>

<p>通过指定exception来指定是否允许重试当前有问题的错误数据，对于一些通过重试可以解决问题的数据是非常有用的，比如更新当前数据到DB时，该条数据被其它进程lock了，则在重试时可能就可以正常更新了。</p>

<p><strong>retry-limit</strong>：用来控制每个item允许重试的次数。</p>

<h3>Rollback</h3>

<p>通过指定exception来忽略rollback操作。对于Skip和Retry，如果Exception是由ItemWriter抛出的，则step中被当前transaction控制的数据会被rollback，因此要配合使用no-rollback-exception-classes来决定是否应该执行rollback。</p>

<p>Step通常会缓存Reader读入的数据，如果发生了rollback则不需要重新读入数据，但是对于一些基于transaction资源的Reader，比如从JMS queue中读取数据的Reader，JMS message也会执行rollback，则需要通过is-reader-transactional-queue来标识不需要缓存读入数据。</p>

<h2>ItemStream</h2>

<p>在Step执行失败需要restart时，可以通过ItemStream获取存储在execution之间状态信息。如果ItemReader，ItemProcessor或者ItemWriter实现了ItemStream接口，则会自动被注册在Spring Context中；否则需要单独注册streams。对于CompositeItemWriter，如果delegate的ItemWriter实现了ItemStream接口，也需要主动注册。</p>

<h2>StepListener</h2>

<p>和ItemStream一样，如果ItemReader，ItemProcessor或者ItemWriter实现了StepListener接口，则会被自动注册。</p>

<h3>StepExecutionListener</h3>

<p><strong>beforeStep</strong> - 在step执行之前调用。<br/>
<strong>afterStep</strong> - 在step结束时调用，不管执行成功或失败。可以在这里更改ExitStatus。</p>

<h3>ChunkListener</h3>

<p><strong>beforeChunk</strong> - 被调用在transaction开始后，但在ItemReader的read方法执行前。<br/>
<strong>afterChunk</strong> - chunk被commit/rollback后被调用。</p>

<p>ChunkListener也可被用在未使用chunk方式的step中，比如Tasklet， 会在tasklet执行前后被调用。</p>

<h3>ItemReadListener</h3>

<p><strong>beforeRead</strong> - 在read方法执行前被调用。<br/>
<strong>afterRead</strong> - 在read方法执行成功后被调用，并返回读到的item作为参数。<br/>
<strong>onReadError</strong> -  在read方法出现异常时被调用，并提供异常的类型作为参数。</p>

<h3>ItemProcessListener</h3>

<p><strong>beforeProcess</strong> - 在ItemProcessor的process方法执行前被调用。<br/>
<strong>afterProcess</strong> - 在process方法执行成功后被调用。<br/>
<strong>onProcessError</strong> - 在process方法出现异常时被调用，并提供item和异常作为参数。</p>

<h3>ItemWriteListener</h3>

<p><strong>beforeWrite</strong> - 在ItemWriter的write方法执行前被调用。<br/>
<strong>afterWrite</strong> - 在ItemWriter的write方法执行成功后被调用。<br/>
<strong>onWriteError</strong> - 在ItemWriter的write方法出现异常时被调用，并提供chunk data和异常作为参数。</p>

<h3>SkipListener</h3>

<p><strong>onSkipInRead</strong> - 当item在读取阶段skip时被调用。 <br/>
<strong>onSkipInProcess</strong> - 当item在process阶段skip时被调用。<br/>
<strong>onSkipInWrite</strong> -  当item在写入阶段skip时被调用，并且在transaction被commit之前。</p>

<h3>Batch Status vs. Exit Status</h3>

<p>Conditional Flow的配置中的on属性使用Exit Status，通常情况下Batch Status和Exit Status是一样的，但StepExecutionListener可以更改Exit Status。</p>

<h3>Step Scope</h3>

<p>使用Job和Step的属性延迟绑定特性时，必须将Bean的scope设置为step。使用该属性可以通过定义Spring Batch的namespace或者定义StepScope。</p>

<p><code>xml applicationContext.xml
   &lt;bean class="org.springframework.batch.core.scope.StepScope" /&gt;
</code></p>

<h2>Resources</h2>

<p>Official site: <a href="http://www.springsource.org/spring-batch">http://www.springsource.org/spring-batch</a></p>

<p>IBM DeveloperWorks - 使用 Spring Batch 构建企业级批处理应用:<br/>
 - <a href="http://www.ibm.com/developerworks/cn/java/j-lo-springbatch1/">http://www.ibm.com/developerworks/cn/java/j-lo-springbatch1/</a><br/>
 - <a href="http://www.ibm.com/developerworks/cn/java/j-lo-springbatch2/">http://www.ibm.com/developerworks/cn/java/j-lo-springbatch2/</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Common Format of CSV Files]]></title>
    <link href="http://askcuix.github.io/blog/2013/03/26/common-format-of-csv-files/"/>
    <updated>2013-03-26T23:43:00+08:00</updated>
    <id>http://askcuix.github.io/blog/2013/03/26/common-format-of-csv-files</id>
    <content type="html"><![CDATA[<p>CSV是一种常见的文件格式，常用于不同系统之间的数据交换，对于该文件格式可以简单描述为：一组使用逗号“,”分隔字段，并以换行符作为一行数据结束的数据集合。该文件可以使用Microsoft Excel查看，但是中文字符会显示为乱码。</p>

<p>这个描述就真的只是简单描述，实际上CSV的格式处理没有这么简单。最近在和另外一个系统做集成，我们的系统会提供CSV数据文件给对方系统处理，由于我们系统中个别字段会包括换行符“CRLF”（这是遗留系统，就不用纠结为什么字段中会包含换行符了），也就是说一个字段可能会显示在几行上，但是这个字段是包含在双引号中的，结果对方系统就处理不了了。因为对方系统只是简单的按行处理数据，用逗号解析字段。</p>

<!-- more -->


<p>这种解析方式通常是没有问题，但是是不完整的。<a href="http://tools.ietf.org/html/rfc4180"><strong>RFC 4180</strong></a>是CSV格式的标准，这里提到并没有官方的规范去定义CSV的格式到底是怎样的，但是根据大多数的实现来看，应该有如下格式：</p>

<ul>
<li><p>每条数据用换行符（CRLF）分割。</p></li>
<li><p>最后一条数据可以不包含换行符。</p></li>
<li><p>可选的header行，如有的话，应该出现在第一行，并且与下面的数据有相同数量的字段。</p></li>
<li><p>在header和每条数据中，使用逗号“,”分隔字段，每一行都应该包含相同数量的字段，空白字符也可作为一个字段，一条数据的最后一个字段不应添加逗号分隔符。</p></li>
<li><p>每个字段可以包含在双引号中。</p></li>
<li><p>如果字段中包含换行符（CRLF），双引号和逗号，则必须将字段包含在双引号中。</p></li>
<li><p>如果字段中包含双引号，则除了需要将字段包含在双引号中，还要在字段中的双引号前再加一个双引号作为转义。</p></li>
</ul>


<p>由此可见，字段中是允许出现换行符的，只要该字段是包含在双引号中的。我也查阅了Spring Batch的CSV Reader的实现，也考虑到了该问题，在发现换行符的同时，也会检查当前字段是否已结束。</p>

<p>因此，我们在生成或解析CSV文件时，应该要考虑到以上特殊字符的处理，在apache的common-lang包中，也有专门针对CSV字符的转义方法，可参考<a href="http://commons.apache.org/proper/commons-lang/javadocs/api-release/org/apache/commons/lang3/StringEscapeUtils.html">StringEscapeUtils</a>，在最大程度上保证系统的robust。</p>
]]></content>
  </entry>
  
</feed>
